<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A short starter to build website for your paper.">
  <meta name="keywords" content="Paper, website">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/phuongnt197">Thu Phuong Nguyen</a><sup>♠️</sup><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/kurone02">Duc M. Nguyen</a><sup>♦️</sup><sup>*</sup>,</span>
            <br>
            <span class="author-block">
              Hyotaek Jeon<sup>♦️</sup>,</span>
            <span class="author-block">
              Huynwook Lee<sup>♦️</sup>,</span>
            <span class="author-block">
              Huynmin Song<sup>♣️</sup>,</span>
            <br>
            <span class="author-block">
              <a href="https://haiv.postech.ac.kr/">Sungahn Ko</a><sup>♦️</sup><sup>**</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/taehwankim">Taehwan Kim</a><sup>♠️</sup><sup>**</sup>.</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>♠️</sup>Ulsan National Institute of Science and Technology,</span>
            <span class="author-block"><sup>♦️</sup>Pohang University of Science and Technology.</span>
            <br>
            <span class="author-block"><sup>*</sup>Equal contribution,</span>
            <span class="author-block"><sup>**</sup>Co-corresponding authors.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/files/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/comming_soon" style="pointer-events: none" disabled
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/comming_soon" style="pointer-events: none" disabled
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/phuongnt197/VEHME"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="./static/images/teaser.png"
                 class="teaser-image"
                />
        <!-- You can also add a video as a teaser -->
        <!--
        <video id="teaser" autoplay muted loop playsinline height="100%">
            <source src="./static/images/teaser.jpg"
                    type="video/mp4">
        </video> 
        -->
      <h2 class="subtitle has-text-centered">
        Our model takes a question, reference answer, and student answer image as input to predict the correctness of the student's solution and identify any error locations, if the solution is incorrect.
      </h2>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/cat.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        An example video to attract people.
      </h2>
    </div> -->
    <!-- <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/video_B.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        A task for you: make this video slot show a dog.
      </h2>
    </div> -->
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Automatically assessing handwritten mathematical solutions is an important problem in educational technology with practical applications, but it remains a significant challenge due to the diverse formats, unstructured layouts, and symbolic complexity of student work. To address this challenge, we introduce VEHME-a <b>V</b>ision-Language Model for <b>E</b>valuating <b>H</b>andwritten <b>M</b>athematics <b>E</b>xpressions—designed to assess open-form handwritten math responses with high accuracy and interpretable reasoning traces. VEHME integrates a two-phase training pipeline: (i) supervised fine-tuning using structured reasoning data, and (ii) reinforcement learning that aligns model outputs with multi-dimensional grading objectives, including correctness, reasoning depth, and error localization. To enhance spatial understanding, we propose an Expression-Aware Visual Prompting Module, trained on our synthesized multi-line math expressions dataset to robustly guide attention in visually heterogeneous inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art performance among open-source models and approaches the accuracy of proprietary systems, demonstrating its potential as a scalable and accessible tool for automated math assessment. Our training and experiment code is publicly available at our <a href="https://github.com/phuongnt197/VEHME">GitHub repository</a>.
          </p>
        </div>
      </div>
    </div>


    <!-- Methodology. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <img src="./static/images/method.png"
                 class="teaser-image"
                />
          <p>
            Our approach to Handwritten Mathematics Grading integrates two key components: (1) supervised fine-tuning using distilled data from the QwQ-32B model to instill foundational reasoning capabilities, and (2) reinforcement learning via Group Relative Policy Optimization to refine the model's ability to generate accurate and explainable grading outputs. As shown in the Figure, this dual-phase training strategy aims to enhance both the correctness assessment and the interpretability of the model's evaluations.
          </p>
        </div>
      </div>
    </div>

    <!-- Experiment Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiment Results</h2>
        <div class="content has-text-justified">
          <img src="./static/images/results.png"
                 class="teaser-image"
                />
          <p>
The experiments are conducted on two tasks--Error Detection and Error Localization, across the AIHub and FERMAT datasets.
The Table above compares our VEHME-Qwen2.5-VL-7B against four open-source baselines and four closed-source systems on the AIHub and FERMAT benchmarks. 
Among the open-source group, our model achieves a clear lead in both Error Detection (ED) and Error Localization (EL). 
On AIHub, VEHME records 73.01% ED accuracy (49.22% F1) and 61.13% EL accuracy (58.18% F1), substantially outperforming Qwen2.5-VL-7B-Instruct (46.68%Acc/ 31.48% F1 for ED, 38.00% Acc/ 33.75% F1 for EL), Pixtral-12B (52.67% / 31.69% ED, 32.20% / 38.35% EL), Phi-4-multimodal-instruct, and Llama-3.2-11B.
This margin highlights the effectiveness of our data synthesis pipeline and dual-phase training in refining both expression recognition and precise localization.
<br>
On FERMAT, open-source models generally struggle, but our approach again leads the pack with 62.61% ED accuracy (29.81% F1) and 31.90% EL accuracy (44.36% F1). 
Notably, Pixtral-12B exhibits an unusual split—high ED accuracy (41.39%) yet very low EL performance (10.53% Acc/3.52% F1)--which echoes the observations in the original FERMAT study: its format alignment and output consistency are suboptimal for localization tasks. 
In contrast, our model's balanced gains across both metrics demonstrate robust adherence to the expected answer format and reliable spatial grounding.
<br>
When we turn to closed-source systems, Gemini-2.0-Flash remains the top performer overall (75.40%/50.30% ED, 67.40%/65.73% EL on AIHub; 75.22%/43.13% ED, 64.24%/75.13% EL on FERMAT), followed closely by GPT-4o and its mini variant. 
These proprietary models, however, exceed tens or even hundreds of billions of parameters, whereas our Qwen2.5-VL-7B backbone contains only 7 billion parameters. 
The result demonstrates that our lightweight model surpasses other open-source approaches and nearly matches closed-source models, while operating at a fraction of the scale of closed systems, underscoring the efficiency gains enabled by our targeted data generation and training regimen.
<br>
The following Figures illustrate qualitative examples of VEHME's grading outputs on AIHub samples.
          </p>
          <img src="./static/images/el_qualitative.png" class="teaser-image"
          />
Qualitative example for Error Detection (ED). Among the 7 models, VEHME provides the most correct
error detection. The model's error is colored in red. The incorrect response from the models can be categorized
into 3 main types: “recognition error”, “problem misunderstanding”, and “hallucination”. Pixtral, GPT, Qwen2.5,
and Gemini all suffer from incorrect OCR of the student's handwritten mathematical expressions. While Pixtral
still produces the correct assessment, its thought was wrong due to incorrect OCR. Phi-4 misinterprets the problem
as solving the inequality, while the problem is asking to find the maximum value of a. Llama3.2 suffers from
hallucination where its thought is correct, but the final assessment is incorrect.
          <img src="./static/images/ed_qualitative.png" class="teaser-image"
          />
Qualitative example for Error Localization (EL). Among the 7 models, VEHME provides the most
correct localization. The model's error is colored in red. The incorrect response from the models can be categorized
into 2 main types: “recognition error” and “student's answer misunderstanding”. Pixtral, Phi-4, and Gemini all
suffer from incorrect OCR of the student's handwritten mathematical expressions. Pixtral and Gemini misread
the inequality 4x ≤ 120 while Phi-4 cannot read beyond the inequality 2x + 5. GPT, Qwen2.5, and Llama3.2
misinterpret the student's answer. GPT and Qwen2.5 think that the student writes 2x + 5 as the perimeter, while
Llama3.2 thinks that the student must determine the number of sides.
        </div>
      </div>
    </div>

    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        
        <h2 class="title is-3">Video</h2>
        <div class="content has-text-justified">
          <p>
            You can add an explanation Youtube video of your paper here!
          </p>
        </div>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/r7jcbH5vq-g?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
        <div class="item item-video0 has-text-centered">
          <h3 class="title is-3">Try it for yourself</h3>
        </div>
          <p style="text-align: center;">
            Check out our code for more details on how to use VEHME:
            <i class="fab fa-github"><a class="icon-link" href="https://github.com/TUM-AVS/paper-website-project" class="external-link"></a></i>
            <a href="https://github.com/phuongnt197/VEHME">Tutorial </a>
          </p>
    </div>
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{nguyen2025vehme,
  title     ={{VEHME}: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions},
  author    ={Thu Phuong Nguyen and Duc M. Nguyen and Hyotaek Jeon and Hyunwook Lee and Hyunmin Song and Sungahn Ko and Taehwan Kim},
  booktitle ={The 2025 Conference on Empirical Methods in Natural Language Processing},
  year      ={2025},
  url       ={https://openreview.net/forum?id=lLlIXm4KNE}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://vehme.github.io">
          <i class="fas fa-home"></i>
      </a>
      <a class="icon-link"
         href="https://arxiv.org/pdf/2405.12399" style="pointer-events: none" disabled>
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/phuongnt197/vehme" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <center><p>
            This website (<a href="https://github.com/vehme/vehme.github.io">source code</a>) was adapted from the popular <a
            href="https://nerfies.github.io">Nerfies</a> project page and is licensed under a <br><a rel="license"
            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p></center>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>